---
title: "p8015_hw6_ax2173"
output: github_document
---

```{r}  
library(tidyverse)
library(modelr)
library(viridis)
library(mgcv)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = '90%'
)

theme_set(theme_classic() + theme(legend.position = 'bottom'))

options(
  ggplot2.continous.colour = 'viridis_d',
  ggplot2.continous.fill = 'viridis_d'
)

scalr_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```

## Problem 1


## Problem 2

Import the data, and do some initial data manipulations, described as following:

* Create a city_state variable
* Create a binary variable indicating whether the homicide is solved. (1 represents solved, 0 represents unsolved)
* Omit several cities
* Limit analysis those for whom victim_race is white or black
* Change the type of victim_age to numeric

```{r}
homicide_df = 
  read_csv("./data/homicide-data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city, ', ',state),
    whether_solve = ifelse(str_detect(disposition, 'Close'), 1, 0)
  ) %>% 
  select(-city, -state, -disposition) %>% 
  filter(
    !city_state %in% c('Dallas, TX', 'Phoenix, AZ','Kansas City, MO','Tulsa, AL'),
    victim_race %in% c('White','Black')
    ) %>% 
  filter(victim_age != 'Unknown') %>% 
  mutate(victim_age = as.numeric(victim_age))
```

For the city of Baltimore, MD, use the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. Save the output of glm as an R object, and apply the broom::tidy to this object.

```{r}
homicide_Bal_df = 
  homicide_df %>% 
  filter(city_state == 'Baltimore, MD') 

Bal_glm = 
  glm(whether_solve ~ victim_age + victim_sex + victim_race, family = binomial(link = logit), data = homicide_Bal_df) %>% 
  broom::tidy()
```

As for solving homicides comparing male victims to female victims keeping all other variables fixed, we can get these two things:

 * The estimated odds ratio is `r round(exp(Bal_glm$estimate[[3]]),3)`
 * The 95% confidence interval is (`r round(exp(Bal_glm$estimate[[3]]-1.96*Bal_glm$std.error[[3]]),3)`,`r round(exp(Bal_glm$estimate[[3]]+1.96*Bal_glm$std.error[[3]]),3)`)


Now run glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. 

```{r}
homicide_df_models = 
  homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = purrr::map(.x = data, ~glm(whether_solve ~ victim_age + victim_sex + victim_race, family = binomial(link = logit), data = .x)),
    result = purrr::map(models, broom::tidy)
    ) %>% 
  select(-data, -models) %>% 
  unnest(result) %>% 
  filter(term == 'victim_sexMale') %>% 
  mutate(
    odds_estimate = exp(estimate),
    CI_lowerbond = exp(estimate - 1.96 * std.error),
    CI_upperbond = exp(estimate + 1.96 * std.error),
  ) %>% 
  select(city_state, estimate, CI_lowerbond, CI_upperbond) 

homicide_df_models %>% knitr::kable(digits = 3)
```

Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.

```{r fig.width=9, fig.height=6}
homicide_df_models %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(city_state, estimate, color = city_state)) +
  geom_linerange(aes(ymin = CI_lowerbond, ymax = CI_upperbond)) +
  geom_errorbar(aes(ymin = CI_lowerbond, ymax = CI_upperbond), width = 0.8) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5)) +
  theme(legend.position = 'none') +
  labs(
    title = 'The Estimated ORs and CIs for Each City',
    x = 'Cities and states',
    y = 'Estimated ORs and CIs',
    caption = 'UNI: ax2173'
  )
```

Comments on this plot:

* The lowest estimated OR goes to New York, NY
* The highest estimated OR goes to Fresno, CA
* The widest CI goes to Long Beach, CA
* The narrowest CI goes to Chicago, IL

## Problem 3

Import the data, load and clean the data for regression analysis.

```{r}
weight_df = 
  read_csv('./data/birthweight.csv') %>% 
  janitor::clean_names() %>% 
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    mrace = as.factor(mrace),
    malform = as.factor(malform)
    ) %>% 
  drop_na()
```

Propose a regression model for birthweight, and the modeling process: I make some hypothesis about these factors to effect the birth weight:

* fincome (family monthly income): the family richer, more nutrition the mothers get;
* malform (presence of malformations): the presence of malformations tend to effect the birth weight;
* pnumlbw (previous number of low birth weight babies): the more previous number of low birth weight babies, the larger probability this baby present the low birth weight;
* smoken (average number of cigarettes smoked per day during pregnancy): smoking during pregnancy tend to effect the birth weigh.

```{r}
bw_reg_0 = lm(bwt ~ fincome + malform + pnumlbw + smoken, data = weight_df)
```

Show a plot of model residuals against fitted values â€“ use add_predictions and add_residuals in making this plot.

```{r fig.width=6, fig.height=6}
weight_df %>% 
  add_predictions(bw_reg_0) %>% 
  add_residuals(bw_reg_0) %>% 
  ggplot(aes(x = resid, y = pred)) +
  geom_point(alpha = .5) +
  labs(
    title = 'Model Residuals against Fitted Values',
    x = 'Residuals',
    y = 'Fitted Values',
    caption = 'UNI: ax2173'
  )
```

Construct two other models to make some comparisons between them.

```{r}
bw_reg_1 = lm(bwt ~ blength + gaweeks, data = weight_df)
bw_reg_2 = lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex + bhead * blength * babysex, data = weight_df)
```

Make this comparison in terms of the cross-validated prediction error; use crossv_mc and functions in purrr as appropriate.

```{r fig.width=6, fig.height=6}
weight_df_comparison = 
  weight_df %>% 
  crossv_mc(100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>% 
  mutate(
    model_0 = map(.x = train, ~lm(bwt ~ fincome + malform + pnumlbw + smoken, data = .x)),
    model_1 = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model_2 = map(.x = train, ~lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex + bhead * blength * babysex, data = .x))
  ) %>% 
  mutate(
    rmse_0 = map2_dbl(.x = model_0, .y = test, ~rmse(model = .x, data = .y)),
    rmse_1 = map2_dbl(.x = model_1, .y = test, ~rmse(model = .x, data = .y)),
    rmse_2 = map2_dbl(.x = model_2, .y = test, ~rmse(model = .x, data = .y))
  )
```

```{r}
weight_df_comparison %>% 
  select(starts_with('rmse')) %>% 
  pivot_longer(
    everything(),
    names_to = 'model',
    values_to = 'rmse',
    names_prefix = 'rmse_'
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() +
  labs(
    title = 'Comparison between Three Models',
    x = 'Model',
    y = 'rmse',
    caption = 'UNI: ax2173'
  )
```

From the chart, we can get the conclusion that my original model performs the worst, and the third model (model_2) performs the best.