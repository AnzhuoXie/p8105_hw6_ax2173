---
title: "p8015_hw5_ax2173"
output: github_document
---

```{r}  
library(tidyverse)
library(rvest)
library(patchwork)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = '90%'
)

theme_set(theme_classic() + theme(legend.position = 'bottom'))

options(
  ggplot2.continous.colour = 'viridis_d',
  ggplot2.continous.fill = 'viridis_d'
)

scalr_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```

## Problem 1
Create a dataframe that includes the list of all files in that directory and the complete path to each file. As a next step, I `map` over paths and import data using the `read_csv` function. 

```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

## Problem 2

Import data. 

```{r}
homicide_df = read_csv("./data/homicide-data.csv")
```

* There are some descriptions about the raw data:
  * There are totally `r ncol(homicide_df)` columns.
  * There are totally `r nrow(homicide_df)` rows.
  * In this dataframe, these variables can be found: `r colnames(homicide_df)`.

Create a city_state variable.

```{r}
homicide_df =
  homicide_df %>% 
  mutate(city_state = str_c(city, ', ',state))
```

Summarize within cities to obtain the total number of homicides and the number of unsolved homicides.

```{r}
count_homicide_df = 
  homicide_df %>%  
  mutate(
    homicide = ifelse(disposition == "Closed without arrest", 1, 0),
    unsolved_homicides = ifelse(disposition == "Open/No arrest", 1, 0)
    ) %>% 
  group_by(city) %>% 
  summarize(
    n_homicides = sum(homicide),
    n_unsolved_homicides = sum(unsolved_homicides),
    n_total = n_homicides + n_unsolved_homicides
   ) 

count_homicide_df %>% 
  knitr::kable()
```

For the city of Baltimore, MD, estimate the proportion of homicides that are unsolved.

```{r}
prop_of_unsolved_homicides = 
  prop.test(1673, 1673 + 152, alternative = c("two.sided"), conf.level = 0.95) %>% 
  broom::tidy()
```

* For the city of Baltimore, MD, the estimated proportion of homicides that are unsolved is `r prop_of_unsolved_homicides[['estimate']]`.
* The confidence interval is (`r prop_of_unsolved_homicides[['conf.low']]`, `r prop_of_unsolved_homicides[['conf.high']]`)

Now run prop.test for each of the cities in dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. 

* NOTICE!!! 
  * The estimate column shows the required proportion of unsolved homicides.
  * The conf.low column shows the required the lower limit of CI of each city.
  * The conf.high column shows the required the upper limit of CI of each city.
  
```{r}
prop_test = function(data) {
  
  prop.test(data$n_unsolved_homicides, data$n_total, alternative = c("two.sided"), conf.level = 0.95) 
}

count_homicide_df = 
  count_homicide_df %>% 
  nest(data = n_homicides : n_total) %>% 
  mutate(
    prop = map(data, prop_test),
    results = map(prop, broom::tidy)
  ) %>% 
  select(-data, -prop) %>% 
  unnest(results) %>% 
  select(city, estimate, conf.low, conf.high)
```

Create a plot that shows the estimates and CIs for each city.

```{r fig.width=9, fig.height=6}
count_homicide_df %>% 
  mutate(city = fct_reorder(city, estimate)) %>% 
  ggplot(aes(city, estimate, color = city)) +
  geom_linerange(aes(ymin = conf.low, ymax = conf.high)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.8) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5)) +
  theme(legend.position = 'none') +
  labs(
    title = 'The proportion and CIs for each city',
    x = 'Cities',
    y = 'Proportion and CIs',
    caption = 'UNI: ax2173'
  )
```

## Problem 3

```{r}
t_test = function(sample_size, mun, sigma) {
  
  x = rnorm(n = sample_size, mean = mun, sd = sigma)
  
  t.test(x, mu = mun, conf.level = 0.95) %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
  
}

t_test_result_0 = 
  rerun(5000, t_test(sample_size = 30, mu = 0, sigma = 5)) %>% 
  bind_rows
```

Repeat the above for mu = {1,2,3,4,5,6}. 

```{r}
t_test_result = 
  tibble(
    mu = c(0, 1, 2, 3, 4, 5, 6)
  ) %>% 
  mutate(
    output_lists = map(.x = mu, ~rerun(5000, t_test(sample_size = 30, mun = .x, sigma = 5))),
    estimate_df = map(output_lists, bind_rows)
  ) %>% 
  select(-output_lists) %>% 
  unnest(estimate_df)
```

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis.
* We can find the association between the effect size and power: under the same sample size, with the increase of effect size, the power grows.

```{r fig.width=9, fig.height=6}
t_test_result %>% 
  mutate(
    if_rejected = ifelse(p.value < 0.05, 1, 0)
      ) %>% 
  group_by(mu) %>% 
  summarize(
    n_rejected = sum(if_rejected),
    prop_rejected = n_rejected / 5000
    ) %>% 
  ggplot(aes(x = mu, y = prop_rejected, color = mu)) +
  geom_point() +
  labs(
    title = 'The proportion of times the null was rejected of each real mean',
    x = 'True value of mean',
    y = 'Rejection proportion',
    caption = 'UNI: ax2173'
  )
```

Make a plot showing the average estimate of μ̂  on the y axis and the true value of μ on the x axis. Make a second plot the average estimate of μ̂  only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. 

Is the sample average of μ̂  across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?

```{r fig.width=9, fig.height=6}
plot_with_all_mu =   
  t_test_result %>% 
  group_by(mu) %>% 
  summarize(
    ave_estimate = mean(estimate)
  ) %>% 
  ggplot(aes(x = mu, y = ave_estimate, color = mu)) +
  geom_point() +
    labs(
    title = 'The average estimate mu vs. true value of mu',
    x = 'The true value of mu',
    y = 'The average estimate of mu',
    caption = 'UNI: ax2173'
  )

plot_with_rejected_mu = 
  t_test_result %>% 
  filter(p.value <= 0.05) %>% 
  group_by(mu) %>% 
  summarize(
    ave_estimate = mean(estimate)
  ) %>% 
  ggplot(aes(x = mu, y = ave_estimate, color = mu)) +
  geom_point() +
    labs(
    title = 'The average estimate rejected mu vs. true value of mu',
    x = 'The true value of mu',
    y = 'The average estimate of rejected mu',
    caption = 'UNI: ax2173'
  )

plot_with_all_mu + plot_with_rejected_mu
```

The sample average of mu across test for which the null is rejected is not approximately equal to the true value of mean. Maybe because if these estimate mu is roughly equal to the true mean, their null will not be rejected.